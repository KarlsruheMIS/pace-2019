\documentclass[twoside,leqno,twocolumn]{article}

\newif\ifFull
%\Fullfalse
\Fulltrue

% Comment out the line below if using A4 paper size
\usepackage[letterpaper]{geometry}

\usepackage{ltexpprt}
\usepackage{t1enc}
\usepackage[utf8]{inputenc}
\usepackage{numprint}
\npdecimalsign{.} % we want . not , in numbers
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{doi}
\usepackage{enumerate}
\usepackage{booktabs}

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\def\MdR{\ensuremath{\mathbb{R}}}
\def\MdN{\ensuremath{\mathbb{N}}}
%\DeclareMathOperator{\sgn}{sgn}
\newcommand{\Id}[1]{\texttt{\detokenize{#1}}}
\newcommand{\Is}       {:=}
\newcommand{\setGilt}[2]{\left\{ #1\sodass #2\right\}}
\newcommand{\sodass}{\,:\,}
\newcommand{\set}[1]{\left\{ #1\right\}}
\newcommand{\gilt}{:}
\newcommand{\ie}{i.\,e.,\xspace}
\newcommand{\eg}{e.\,g.,\xspace}
\newcommand{\etal}{et~al.\xspace}
\newcommand{\Wlog}{w.\,l.\,o.\,g.\ }
\newcommand{\wrt}{w.\,r.\,t.\xspace}
\newcommand{\csch}[1]{{\color{red} CS: #1}}

\newcommand{\mytitle}{WeGotYouCovered: The Winning Solver from the 2019 PACE Implementation Challenge, Vertex Cover Track}

\setlength\parfillskip{0pt plus .4\textwidth}
\setlength\emergencystretch{.1\textwidth}
\clubpenalty10000
\widowpenalty10000
\displaywidowpenalty=10000

\usepackage{changes}
\definechangesauthor[name={Darren Strash}, color=blue]{DS}
\definechangesauthor[name={Demian Hespe}, color=blue]{DH}



%\subjclass{G.2.2 Graph Theory -- Graph Algorithms, G.4 Mathematical Software -- Algorithm Design and Analysis} 
%\keywords{kernelization, branch-and-reduce, local search}
%\EventEditors{}
%\EventNoEds{0}
%\EventLongTitle{}
%\EventShortTitle{PACE 2019}
%\EventAcronym{PACE}
%\EventYear{2019}
%\EventDate{}
%\EventLocation{}
%\EventLogo{}
%\SeriesVolume{}
%\ArticleNo{}

\begin{document}
\title{\mytitle\thanks{
    The research leading to these results has received funding from the European Research Council under the European Community's Seventh Framework Programme (FP7/2007-2013) /ERC grant agreement No. 340506}}
\author{Demian Hespe\thanks{Karlsruhe Institute of Technology, Karlsruhe, Germany} \and Sebastian Lamm\thanks{Karlsruhe Institute of Technology, Karlsruhe, Germany} \and Christian Schulz\thanks{University of Vienna, Faculty of Computer Science, Austria} \and Darren Strash\thanks{Hamilton College, New York, USA}}

%\affil[1]{Karlsruhe Institute of Technology, Karlsruhe, Germany \\
  %\texttt{hespe@kit.edu}}
%\affil[2]{Karlsruhe Institute of Technology, Karlsruhe, Germany\\
  %\texttt{lamm@kit.edu}}
%\affil[3]{University of Vienna, Faculty of Computer Science, Vienna, Austria\\ \texttt{christian.schulz@univie.ac.at}}
%\affil[4]{Hamilton College, New York, USA,  \texttt{dstrash@hamilton.edu}}

\date{}


%\Copyright{}
\maketitle
\begin{abstract}
We present the winning solver of the PACE 2019 Implementation Challenge Vertex Cover Track. The vertex cover problem is one of a handful of problems for which \emph{kernelization}---the repeated reducing of the input size via \emph{data reduction rules}---is known to be highly effective in practice. Our algorithm uses a portfolio of techniques, including an aggressive kernelization strategy with all known reduction rules, local search, branch-and-reduce, and a state-of-the-art branch-and-bound solver. Of particular interest is that several of our techniques were \emph{not} from the literature on the vertex over problem: they were originally published to solve the (complementary) maximum independent set and maximum clique problems. Lastly, we perform extensive experiments to show the impact of the different solver techniques on the number of instances solved during the challenge.
\end{abstract}

%apply an initial aggressive kernelization strategy, using all known reduction rules for the problem. From there we use local search to produce a high-quality solution on the (hopefully smaller) kernel, which we use as a starting solution for a branch-and-bound solver. Our branch-and-bound solver also applies reduction rules via a branch-and-reduce scheme -- applying rules when possible, and branching otherwise -- though this may be toggled to omit reductions if they are not effective.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

A \emph{vertex cover} of a graph $G=(V,E)$ is a set of vertices $S\subseteq V$ of $G$ such that every edge of $G$ has at least one of member of $S$ as an endpoint (i.e., $\forall (u,v) \in E\,\, [u\in S \textrm{ or } v \in S]$).
A minimum vertex cover is a vertex cover of minimum cardinality. 
Complementary to vertex covers are independent sets and cliques. An independent set is a set of vertices $I\subseteq V$, all pairs of which are not adjacent, and an clique is a set of vertices $K\subseteq V$ all pairs of which are adjacent. A maximum independent set (maximum clique) is an independent set (clique) of maximum cardinality. The goal of the maximum independent set problem (maximum clique problem) is to compute a maximum independent set (maximum clique).

Many techniques have been proposed for solving these problems, and papers in the literature usually focus on one of these problems in particular. However, all of these problems are equivalent: a
minimum vertex cover $C$ in $G$ is the complement of a maximum independent set $V\setminus C$ in $G$, which is a maximum clique $V\setminus C$ in $\overline{G}$. Thus, an algorithm that solves one of these problems can be used to~solve~the~others.
For our approach, we use a portfolio of solvers, using techniques from the literature on all three problems. These include data reduction rules and branch-and-reduce for the minimum vertex cover problem~\cite{akiba-tcs-2016}, iterated local search for the maximum independent set problem~\cite{andrade-2012}, and a state-of-the-art branch-and-bound maximum clique solver~\cite{DBLP:journals/cor/LiJM17}.

We first briefly describe related work. Then we outline each of the techniques that we use, and finally describe how we combine all of the techniques in our final solver that scored most of the points during the PACE 2019 Implementation Challenge. Lastly, we perform an experimental evaluation to show the impact of the components used on the final number of instances solved during the challenge.
Experiments emphasize that data reductions play an important role to boost the
performance of a branch-and-bound clique solver, and local search is highly
effective to boost the performance of a branch-and-reduce solver for the
independent~set~problem. \csch{somehow emphasize that nothing like this has been
  done before}\comment[id=DH]{But it has for cliques (according to Section 2)}


\section{Related Work}
\label{sec:related_work}
We now present important related work. 
Research results in the area can be found through work on the minimum vertex cover problem and its complementary maximum clique and independent set problems, and can often be categorized depending on the angle of attack. In practice, the maximum clique problem is normally solved, for exact exponential (theoretical) algorithms, the maximum independent set problem is considered, and for parameterized algorithms, the minimum vertex cover problem is considered. However, these problems are only \emph{trivially} different --- techniques for solving one problem require only subtle modification to solve the other two.

\paragraph*{Exponential-time Algorithms.}
The best known algorithms for solving the independent set problem on general graphs have exponential running time, and much research has be devoted to reducing the base of the exponent. The main technique is  to develop rules to modify the graph, removing or contracting subgraphs that can be solved simply, which reduces the graph to a smaller instance. These rules are referred to as \emph{data reduction rules} (often simplified to \emph{reduction rules} or \emph{reductions}).
Reduction rules have been used to reduce the running time of the brute force $O(n^22^n)$ algorithm to the $O(2^{n/3})$ time algorithm of Tarjan and Trojanowski~\cite{tarjan-1977}, and to the current best polynomial space algorithm with running time of $O^*(1.1996^n)$ by Xiao and Nagamochi~\cite{xiao2017exact}. 
The reduction rules used for these algorithms are often staggeringly simple, including \emph{pendant vertex removal}, \emph{vertex folding}~\cite{chen-1999} and \emph{twin} reductions~\cite{Xiao201392}, which eliminate nearly all vertices of degree three or less from the graph. 
These algorithms apply reductions during recursion, only branching when the graph can no longer be reduced~\cite{fomin-2010}, and are often referred to as \emph{branch-and-reduce} algorithm. Further techniques used to accelerate these algorithms include \emph{branching rules}~\cite{} which eliminate unnecessary branches from the search tree, as well as faster exponential-time graphs for graphs of degree 3 or less~\cite{}.

\paragraph*{Parameterized Algorithms.}
The most efficient algorithms for computing a minimum vertex cover in both theory and practice repeatedly apply data reduction rules to obtain a (hopefully) much smaller problem instance. If this smaller instance has size bounded by a function of some parameter, it's~called~a~\emph{kernel}, and producing a polynomially-sized kernel gives a fixed-parameter tractable in the chosen parameter. Reductions are surprisingly effective for the minimum vertex cover problem. In particular, letting $k$ be the size of minimum vertex cover, the well-known crown reduction rule produces a kernel of size $3k$~\cite{chor2005linear} and the LP-relaxation reduction due to Nemhauser and Trotter~\cite{nemhauser-1975}, produces a kernel that is within a constant factor of the minimum vertex cover size $2k$~\cite{chen1999}.

\comment[id=DS]{some parameterized results for minimum vertex cover.}
For more information on the history of vertex cover kernelization, see the recent survey by Fellows et al.~\cite{fellows2018known}.

\paragraph*{Exact Algorithms in Practice.}
The most efficient maximum clique solvers use a branch-and-bound search with advanced vertex reordering strategies and pruning (typically using approximation algorithms for graph coloring, MaxSAT~\cite{li-maxsat-2013} or constraint satisfaction). The long-standing canonical algorithms for finding the maximum clique are the MCS algorithm by Tomita et al.~\cite{tomita-recoloring} and the bit-parallel algorithms of San Segundo et al.~\cite{segundo-recoloring,segundo-bitboard-2011}. However, recently Li et al.~\cite{DBLP:journals/cor/LiJM17} introduced the MoMC algorithm, which introduces incremental MaxSAT logic to achieve speed ups of up to 1,000 over MCS. Experiments by Batsyn et al.~\cite{batsyn-mcs-ils-2014} show that MCS can be sped up significantly by giving an initial solution found through local search. However, even with these state-of-the-art algorithms, graphs on thousands of vertices remain intractable. For example, a difficult graph on 4,000 required 39 wall-clock hours in a highly-parallel MapReduce cluster, and is estimated to require over a year of sequential computation~\cite{xiang-2013}. A thorough discussion of many results in clique finding can be found in the survey of Wu and Hao~\cite{wu-hao-2015}.\comment[id=DS]{Mention reductions used to speed up maximum clique solvers? Buchanen; San Segundo; Chang}

Data reductions have been successfully applied in practice to solve exact problems that are intractable with general algorithms. Butenko et al.~\cite{butenko-2002} were the first to show that simple reductions could be used to compute exact maximum independent sets on graphs with hundreds vertices for graphs derived from error-correcting codes. Their algorithm works by first applying \emph{isolated clique removal} reductions, then solving the remaining graph with a branch-and-bound algorithm. Later, Butenko and Trukhanov~\cite{butenko-trukhanov} further showed that applying a \emph{critical set} reduction could be used to solve graphs produced by the Sanchis graph generator.
Larson~\cite{larson-2007} later proposed an algorithm to find a \emph{maximum} critical independent set, but in experiments it proved to be slow in practice~\cite{strash2016power}.
Later Iwata~\etal~\cite{iwata-2014} showed how to remove a large collection of vertices from a maximum matching all at once; however, it is not known if these reductions are equivalent.

For the minimum vertex cover, it has long been known that two such simple reductions, called \emph{pendant vertex removal} and \emph{vertex folding}, are particularly effective in practice. However, two seminal experimental works explored the efficacy of further reductions. Abu-Khzam et al.~\cite{abu-khzam-2007} showed that \emph{crown reductions} are effective in practice on \added[id=DS]{something... how effective?}{} graphs. We briefly note that \emph{crowns} are also critical sets\comment[id=DS]{check}, and thus in some ways this work replicates that of Butenko and Trukhanov~\cite{butenko-trukhanov}, though their experiments are run on different graphs.


\comment[id=DS]{discuss crown reduction connection to LP relaxation, cite new paper.}

Later, Akiba and Iwata~\cite{akiba-tcs-2016} showed that an extensive collection of advanced data reduction rules (together with branching rules) are also highly effective in practice. Their algorithm finds exact minimum vertex covers on a corpus of large social networks with hundreds of thousands of vertices or more in mere seconds. More details on the reduction rules follow in Section~\ref{sec:techniques}.

\comment[id=DS]{Pay homage to new reduction rules, and further experiments.}

\paragraph*{Inexact Algorithms.}
\comment[id=DS]{Needed?}


\section{Techniques}
\label{sec:techniques}
We now describe techniques that we use in~our~solver.
\subsection{Kernelization.}
The most efficient algorithms for computing a minimum vertex cover in both theory and practice use \emph{data reduction rules} to obtain a much smaller problem instance. If this smaller instance has size bounded by a function of some parameter, it's~called~a~\emph{kernel}. 

We use an extensive (though not exhaustive) collection of data reduction rules whose efficacy was studied by Akiba and Iwata~\cite{akiba-tcs-2016}. To compute a kernel, Akiba and Iwata~\cite{akiba-tcs-2016} apply their
reductions~$r_1, \dots, r_j$ by iterating over all reductions and trying to
apply the current reduction $r_i$ to all vertices. If $r_i$ reduces at
least one vertex, they restart with reduction~$r_1$. When reduction~$r_j$ 
is executed, but does not reduce any vertex, all reductions have been applied
exhaustively, and a kernel is found. Following their study we order the reductions
as follows: degree-one vertex (i.e., pendant) removal, vertex folding~\cite{chen-1999}, a well-known linear-programming 
relaxation~\cite{iwata-2014,nemhauser-1975} related to crown removal~\cite{abu-khzam-2007}, unconfined vertex removal~\cite{Xiao201392}, and twin, funnel, and desk reductions~\cite{Xiao201392}.
To be self-contained, we now give a brief description of those reductions, in order of increasing complexity. Each reduction allows us to choose vertices that are either in some minimum vertex cover, or for which we can locally choose a vertex in a minimum vertex cover after solving the remaining graph, by following simple rules. If a minimum vertex cover  is found on the kernel graph $\mathcal{K}$, then each reduction may be undone, producing an vertex cover in the original graph. Refer to Akiba and Iwata~\cite{akiba-tcs-2016} for a more thorough discussion, including implementation details. We use our own implementation of the reduction algorithms in our method which is very similar to the original Java implementation. \\

\noindent\textbf{Pendant vertices:} Any vertex $v$ of degree one, called a \emph{pendant}, is in some MIS; therefore $v$ and its neighbor $u$ can be removed from $G$. \\
%We can see this as follows: Either $v$'s neighbor is in a MIS $I$, or not. Suppose it is not in $I$, then $v$ must be in $I$. Suppose it is, then it can be removed from the MIS, and add $v$.

%\noindent\textbf{Dominance:} If there exist two vertices $u$ and $v$, such that $N(u) = N(v) \cup \{v\}$ then in any independent set $\mathcal{I}$ either both $v$ and $u$ will be excluded from $\mathcal{I}$, or exactly one of $u$ or $v$ will be in $\mathcal{I}$. Therefore, we may remove either $v$ or $u$ from the graph.

\noindent\textbf{Vertex folding:} For a vertex $v$ with degree 2 whose neighbors $u$ and $w$ are not adjacent, either $v$ is in some MIS, or both $u$ and $w$ are in some MIS. Therefore, we can contract $u$, $v$, and $w$ to a single vertex $v'$ and decide which vertices are in the MIS later. \\
%If $v'$ is in the computed MIS, then $u$ and $w$ are added to the independent set, otherwise $v$ is added. Thus, a vertex fold contributes an additional vertex to an independent set.

\ifFull
\noindent\textbf{Linear Programming:}
First introduced by Nemhauser and Trotter~\cite{nemhauser-1975} for the vertex packing problem, they present a linear programming relaxation with a half-integral solution (i.e., using only values 0, 1/2, and 1) which can be solved using bipartite matching. Their relaxation may be formulated for the independent set problem as follows: maximize $\sum_{v\in V}{x_v}$, such at for each edge $(u, v) \in E$, $x_u + x_v \leq 1$ and for each vertex $v \in V$, $x_v \geq 0$. Vertices with value 1 must be in the MIS, and therefore are added to the solution. We use the further improvement from Iwata, Oka, and Yoshida~\cite{iwata-2014}, which computes a solution whose half-integral part is minimal. \\
\else
\noindent\textbf{Linear Programming:}
A well-known~\cite{nemhauser-1975} linear programming relaxation for the MIS problem with a half-integral solution (i.e., using only values 0, 1/2, and 1) can be solved using bipartite matching: maximize $\sum_{v\in V}{x_v}$ such that $\forall (u, v) \in E$, $x_u + x_v \leq 1$ and $\forall v \in V$, $x_v \geq 0$. Vertices with value 1 must be in the MIS and can thus be removed from $G$ along with their neighbors. We use an improved version~\cite{iwata-2014} that computes a solution whose half-integral part is minimal.\\ % the further improvement from Iwata, Oka, and Yoshida
\fi

\ifFull
\noindent\textbf{Unconfined:} Developed by Xiao and Nagamochi~\cite{Xiao201392}, the unconfined reduction is a generalization of domination and \emph{satellite} reduction rules. A vertex $v$ is said to be unconfined if there exists a set $S$, such that $v\in S$ and $\exists u\in S$ such that $|N(u)\cap S| = 1$ and $N(u) \setminus N[S]$ is empty. Such a vertex is never in a MIS, so it can be removed from the graph.  \\
\else
%\noindent\textbf{Unconfined~\cite{Xiao201392}:} A vertex $v$ is \emph{unconfined} if there exists a set $S\subseteq V$, such that $v\in S$ and $\exists u\in S$ such that $|N(u)\cap S| = 1$ and $N(u) \setminus N[S]$ is empty. Such a vertex is never in any MIS, so it can be removed from $G$. \strash{Double check logic.}
\noindent\textbf{Unconfined~\cite{Xiao201392}:} Though there are several definitions of \emph{unconfined} vertex in the literature, we use the simple one from Akiba and Iwata~\cite{akiba-tcs-2016}. A vertex $v$ is \emph{unconfined} when determined by the following simple algorithm. First, initialize $S = \{v\}$. Then find a $u \in N(S)$ such that $|N(u) \cap S| = 1$ and $|N(u) \setminus N[S]|$ is minimized. If there is no such vertex, then $v$ is confined. If $N(u) \setminus N[S] = \emptyset$, then $v$ is unconfined.  If $N(u)\setminus N[S]$ is a single vertex $w$, then add $w$ to $S$ and repeat the algorithm. Otherwise, $v$ is confined. Unconfined vertices can be removed from the graph, since there always exists an MIS $\mathcal{I}$ that contains no unconfined vertices. \\
\fi

\ifFull
\noindent\textbf{Twin:} This is a generalization of the vertex folding rule. Suppose there are two vertices $u$ and $v$ that have degree 3 and share the same neighborhood. If $u$'s neighborhood $N(u)$ induces a graph with edges, then $u$ and $v$ are added to the independent set and $u$, $v$, and their neighborhoods are removed from the graph. Otherwise, vertices in $N(u)$ may belong in the independent set. We still remove $u$, $v$, and their neighborhoods, and add a new gadget vertex $w$ to the graph with edges to $u$'s two-neighborhood (vertices at a distance 2 from $u$). If $w$ is in some MIS, none of $u$'s two-neighbors are in the independent set, and therefore $N(u)$ is part of the independent set. Otherwise, if $w$ is not in the MIS, then some of $u$'s two-neighbors are in the independent set, and therefore $u$ and $v$ are added to the independent set. Thus, the twin reduction adds an additional two vertices to the computed~independent~set. \\
\else
\noindent\textbf{Twin~\cite{Xiao201392}:} Let $u$ and $v$ be vertices of degree 3 with $N(u) = N(v)$. If $G[N(u)]$ has edges, then add $u$ and $v$ to $\mathcal{I}$ and remove $u$, $v$, $N(u)$, $N(v)$ from $G$. Otherwise, some vertices in $N(u)$ may belong to some MIS $\mathcal{I}$. We still remove $u$, $v$, $N(u)$ and $N(v)$ from $G$, and add a new gadget vertex $w$ to $G$ with edges to $u$'s two-neighborhood (vertices at a distance 2 from $u$). If $w$ is in the computed MIS, then none of $u$'s two-neighbors are $\mathcal{I}$, and therefore $N(u) \subseteq \mathcal{I}$. Otherwise, if $w$ is not in the computed MIS, then some of $u$'s two-neighbors are in $\mathcal{I}$, and therefore $u$ and $v$ are added to $\mathcal{I}$.\\
\fi

\noindent\textbf{Alternative:} Two sets of vertices $A$ and $B$ are set to be \emph{alternatives} if $|A| = |B| \geq 1$ and there exists an MIS $\mathcal{I}$ such that $\mathcal{I}\cap(A\cup B)$ is either $A$ or $B$. Then we remove $A$ and $B$ and $C = N(A)\cap N(B)$ from $G$ and add edges from each $a \in N(A)\setminus C$ to each $b\in N(B)\setminus C$.
Then we add either $A$ or $B$ to $\mathcal{I}$, depending on which neighborhood has vertices in $\mathcal{I}$. Two structures are detected as alternatives. First, if $N(v)\setminus \{u\}$ induces a complete graph, then $\{u\}$ and $\{v\}$ are alternatives (a \emph{funnel}). Next, if there is a cordless 4-cycle $a_1b_1a_2b_2$ where each vertex has at least degree 3. Then sets $A=\{a_1, a_2\}$ and $B=\{b_1, b_2\}$ are alternatives (called a \emph{desk}) when $|N(A) \setminus B| \leq 2$, $|N(A) \setminus B| \leq 2$, and $N(A) \cap N(B) = \emptyset$. 




\comment[id=DS]{Describe funnel and desk? How they implement Alternative. CS: integrated desk above, funnel has already been there }

\subsection{Branch-and-Reduce.}
Branch-and-reduce is a paradigm that intermixes data reduction rules and branching. We use the algorithm of Akiba and Iwata, which exhaustively applies their full suite of reduction rules before branching, and includes a number of advanced branching rules. When branching, a vertex is chosen at random for inclusion into the vertex cover.

\comment[id=DS]{Discuss Satellite and Mirror branching rules}

\comment[id=DS]{lower bounds}

\subsection{Branch-and-Bound.} Experiments by Strash \cite{strash2016power} show that the full power of branch-and-reduce is only needed \emph{very rarely} in real-world instances; kernelization followed by standard branch-and-bound solver is sufficient for many real-world instances. Furthermore, branch-and-reduce does not work well on many synthetic benchmark instances, where data reduction rules are ineffective~\cite{akiba-tcs-2016}, and instead add significant overhead to branch-and-bound. We use a state-of-the-art branch-and-bound maximum clique solver (MoMC) by Li et al.~\cite{DBLP:journals/cor/LiJM17}, which uses incremental MaxSAT reasoning to prune search, and a combination of static and dynamic vertex ordering to select the vertex for branching. We run the clique solver on the complement graph, giving a maximum independent set from which we derive a minimum vertex cover. In preliminary experiments, we found that a kernel can sometimes be harder for the solver than the original input; therefore, we run the algorithm on both the kernel and on the original graph.

\subsection{Iterated Local Search.}
Batsyn et al.~\cite{batsyn-mcs-ils-2014} showed that if branch-and-bound search is primed with a high-quality solution from local search, then instances can be solved up to thousands of times faster. 
We use the iterated local search algorithm by Andrade et al.~\cite{andrade-2012} to prime the branch-and-reduce solver with a high-quality initial solution. Iterated local search was originally implemented for the maximum independent set problem, and is based on the notion of $(j,k)$-swaps. A $(j,k)$-swap removes $j$ nodes from the current solution and inserts $k$ nodes. The authors present a fast linear-time implementation that, given a maximal independent set, can find a $(1,2)$-swap or prove that none exists. Their algorithm applies $(1,2)$-swaps until reaching a local maximum, then perturbs the solution and repeats. We implemented the algorithm to find a high-quality solution on \emph{the kernel}. Calling local search on the kernel has been shown to produce a high-quality solution much faster than without kernelization~\cite{chang2017computing,dahlum2016accelerating}.

\section{Putting it all Together}
Our algorithm first runs a preprocessing phase, followed by 4 phases of solvers.

%Our solver is a combination of different kernelization techniques \cite{DBLP:conf/alenex/Hespe0S18}, local search~\cite{DBLP:conf/wea/AndradeRW08}, as well as branch-and-reduce~\cite{akiba-tcs-2016,DBLP:journals/cor/LiJM17}.

%
%Our algorithm uses a portfolio of solvers, i.e., a branch-and-bound solver for vertex cover~\cite{akiba-tcs-2016} as well as a branch-and-bound solver for the maximum clique problem \cite{DBLP:journals/cor/LiJM17}.

\begin{description}
\item[Phase 1. (Preprocessing)] Our algorithm starts by computing a kernel of the graph using the reductions by Akiba and Iwata~\cite{akiba-tcs-2016}. 
From there we use iterated local search to produce a high-quality solution $S_{\textrm{init}}$ on the (hopefully smaller) kernel. 
\item[Phase 2. (Branch-and-Reduce, short)]
We prime a branch-and-reduce solver with the initial solution $S_{\textrm{init}}$ and run it with a short time limit.
\item[Phase 3. (Branch-and-Bound, short)]
If Phase 2 is unsuccessful, we run the MoMC~\cite{DBLP:journals/cor/LiJM17} clique solver on the complement of the kernel, also using a short time limit. Sometimes kernelization can make the problem harder for MoMC. Therefore, if the first call was unsuccessful we also run MoMC on the complement of the original (unkernelized) input with the same short time limit.

\item[Phase 4. (Branch-and-Reduce, long)]
If we have still not found a solution, we run branch-and-reduce on the kernel using initial solution $S_{\textrm{init}}$ and a longer time limit. We opt for this second phase because, while most graphs amenable to reductions are solved very quickly with branch-and-reduce (less than a second),
experiments by Akiba and Iwata~\cite{akiba-tcs-2016} showed that other slower instances either finish in at most a few minutes, or take significantly longer---more than the time limit allotted for the challenge. This second phase of branch-and-reduce is meant to catch any instances that still benefit from reductions.

\item[Phase 5. (Branch-and-Bound, remaining time)]
If all previous phases were unsuccessful, we run MoMC on the original (unkernelized) input graph until the end of the time given to the program by the challenge. This is meant to capture only the most hard-to-compute instances.
\end{description}

The ordering and time limits were carefully chosen so that the overall algorithm outputs solutions of the ``easy'' instances \emph{quickly}, while still being able to solve hard instances.
\vfill\pagebreak
\section{Experimental Results}
We now look at the impact of the algorithmic components on the number of instances solved.
Here, we use the public instances -- obtained from \url{https://pacechallenge.org/files/pace2019-vc-exact-public-v2.tar.bz2} -- of the PACE 2019 Track A implementation challenge. This set contains 100 instances overall. Afterwards, we present the results comparing against the second and third best competing algorithms during the challenge. 

\subsection{Methodology and Setup.}
All of our experiments were run on a machine with  four Sixteen-Core Intel Xeon Haswell-EX E7-8867 processors running at $2.5$ GHz, $1$ TB of main memory, and $32768$ KB L2-Cache.
The machine runs Debian GNU/Linux 9 and Linux kernel version 4.9.0-9.
All algorithms were implemented in C++11 and compiled with gcc~version 6.3.0 with optimization flag \texttt{-O3}.
Each algorithm was run sequentially with a time limit of 30 minutes. Our evaluations focus on the total number of instances solved.
\subsection{Evaluation.}
We now explain our main configuration that we use in our experimental setup.
In the following \texttt{MoMC} runs the clique solver~\cite{DBLP:journals/cor/LiJM17} on the complement of the input graph, \texttt{RMoMC} applies reductions to the input graph exhaustively and then runs MoMC on the complement of the kernel graph, \texttt{LSBnR} applies reductions exhaustively, then runs local search to obtain a high-quality solution on the kernel which is used as a initial bound in the branch-and-reduce algorithm that is run on the kernel, \texttt{BnR} applies reductions and then runs the branch-and-reduce algorithm on the kernel (no local search is used to improve an initial bound), \texttt{FullA} is the full algorithm as described above.


%\section{Material}
%\begin{itemize}
        %\item MoMC solves 30 / 100 instances 
        %\item Kernel + MoMC solves 68 / 100 instances 
        %\item Kernel + BnR solves 55 / 100 instances 
        %\item Kernel + BnR - Local Search solves 42 / 100 instances 
        %\item Full solver solves 82 / 100 instances 
%\end{itemize}

%\begin{description}
%\item[GitHub:] \url{https://github.com/sebalamm/pace-2019/releases/tag/pace-2019}
%\item[DOI:] \url{https://doi.org/10.5281/zenodo.2816116}
%\end{description}
%\section{TODOs}
%\begin{itemize}
        %\item TODO add more exact stuff, heuristics?
%\item TODO insert final three solvers of pace challenge
%\item create a table with instances solved
%\end{itemize}
\begin{table*}
\centering
\caption{Detailed per instance results. The columns $n$,$m$ refer to the number of nodes and edges of the input graph, $n'$,$m'$ refer to the number of nodes and edges of the kernel graph after reductions have been applied exhaustively, and $|VC|$ refers to the size of the optimal vertex cover of the input graph. With X we denote if a solver has been solving an instance, and with - we denote if this has not been the case.}
\label{tab:detailedresults1}
\begin{tabular}{l@{\hskip 25pt} rrrr|ccccc|rc}
\toprule
inst\# & $n$ &$m$& $n'$& $m'$ & \texttt{MoMC} & \texttt{RMoMC} & \texttt{LSBnR} & \texttt{BnR} & \texttt{FullA} & $|VC|$ \\
                \midrule

001 &\numprint{6160}&\numprint{40207}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{2586}&\\ 
003 &\numprint{60541}&\numprint{74220}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{12190}&\\ 
005 &\numprint{200}&\numprint{819}&\numprint{192}&\numprint{800}&X&X&X&X&X&  \numprint{129}&\\ 
007 &\numprint{8794}&\numprint{10130}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{4397}&\\ 
009 &\numprint{38452}&\numprint{174645}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{21348}&\\ 
011 &\numprint{9877}&\numprint{25973}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{4981}&\\ 
013 &\numprint{45307}&\numprint{55440}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{8610}&\\ 
015 &\numprint{53610}&\numprint{65952}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{10670}&\\ 
017 &\numprint{23541}&\numprint{51747}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{12082}&\\ 
019 &\numprint{200}&\numprint{884}&\numprint{194}&\numprint{862}&X&X&X&X&X&  \numprint{130}&\\ 
021 &\numprint{24765}&\numprint{30242}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{5110}&\\ 
023 &\numprint{27717}&\numprint{133665}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{16013}&\\ 
025 &\numprint{23194}&\numprint{28221}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{4899}&\\ 
027 &\numprint{65866}&\numprint{81245}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{13431}&\\ 
029 &\numprint{13431}&\numprint{21999}&\numprint{0}&\numprint{0}&-&X&X&X&X&  \numprint{6622}&\\ 
031 &\numprint{200}&\numprint{813}&\numprint{198}&\numprint{818}&X&X&X&X&X&  \numprint{136}&\\ 
033 &\numprint{4410}&\numprint{6885}&\numprint{138}&\numprint{471}&-&X&X&X&X&  \numprint{2725}&\\ 
035 &\numprint{200}&\numprint{884}&\numprint{189}&\numprint{859}&X&X&X&X&X&  \numprint{133}&\\ 
037 &\numprint{198}&\numprint{824}&\numprint{194}&\numprint{810}&X&X&X&X&X&  \numprint{131}&\\ 
039 &\numprint{6795}&\numprint{10620}&\numprint{219}&\numprint{753}&-&X&X&X&X&  \numprint{4200}&\\ 
041 &\numprint{200}&\numprint{1040}&\numprint{200}&\numprint{1023}&X&X&X&X&X&  \numprint{139}&\\ 
043 &\numprint{200}&\numprint{841}&\numprint{198}&\numprint{844}&X&X&X&X&X&  \numprint{139}&\\ 
045 &\numprint{200}&\numprint{1044}&\numprint{200}&\numprint{1020}&X&X&X&X&X&  \numprint{137}&\\ 
047 &\numprint{200}&\numprint{1120}&\numprint{198}&\numprint{1080}&X&X&X&X&X&  \numprint{140}&\\ 
049 &\numprint{200}&\numprint{957}&\numprint{198}&\numprint{930}&X&X&X&X&X&  \numprint{136}&\\ 
051 &\numprint{200}&\numprint{1135}&\numprint{200}&\numprint{1098}&X&X&X&X&X&  \numprint{140}&\\ 
053 &\numprint{200}&\numprint{1062}&\numprint{200}&\numprint{1026}&X&X&X&X&X&  \numprint{139}&\\ 
055 &\numprint{200}&\numprint{958}&\numprint{194}&\numprint{938}&X&X&X&X&X&  \numprint{134}&\\ 
057 &\numprint{200}&\numprint{1200}&\numprint{197}&\numprint{1139}&X&X&X&X&X&  \numprint{142}&\\ 
059 &\numprint{200}&\numprint{988}&\numprint{193}&\numprint{954}&X&X&X&X&X&  \numprint{137}&\\ 
061 &\numprint{200}&\numprint{952}&\numprint{198}&\numprint{914}&X&X&X&X&X&  \numprint{135}&\\ 
063 &\numprint{200}&\numprint{1040}&\numprint{200}&\numprint{1011}&X&X&X&X&X&  \numprint{138}&\\ 
065 &\numprint{200}&\numprint{1037}&\numprint{200}&\numprint{1011}&X&X&X&X&X&  \numprint{138}&\\ 
067 &\numprint{200}&\numprint{1201}&\numprint{200}&\numprint{1174}&X&X&X&X&X&  \numprint{143}&\\ 
069 &\numprint{200}&\numprint{1120}&\numprint{196}&\numprint{1077}&X&X&X&X&X&  \numprint{140}&\\ 
071 &\numprint{200}&\numprint{984}&\numprint{200}&\numprint{952}&X&X&X&X&X&  \numprint{136}&\\ 
073 &\numprint{200}&\numprint{1107}&\numprint{200}&\numprint{1078}&X&X&X&X&X&  \numprint{139}&\\ 
075 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
077 &\numprint{200}&\numprint{988}&\numprint{193}&\numprint{954}&X&X&X&X&X&  \numprint{137}&\\ 
079 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
081 &\numprint{199}&\numprint{1124}&\numprint{197}&\numprint{1087}&X&X&X&X&X&  \numprint{141}&\\ 
083 &\numprint{200}&\numprint{1215}&\numprint{198}&\numprint{1182}&X&X&X&X&X&  \numprint{144}&\\ 
085 &\numprint{11470}&\numprint{17408}&\numprint{3539}&\numprint{25955}&-&-&-&-&-&  &\\ 
087 &\numprint{13590}&\numprint{21240}&\numprint{441}&\numprint{1512}&-&X&-&-&X&  \numprint{8400}&\\ 
089 &\numprint{57316}&\numprint{77978}&\numprint{16834}&\numprint{54847}&-&-&-&-&-&  &\\ 
091 &\numprint{200}&\numprint{1196}&\numprint{200}&\numprint{1163}&X&X&X&X&X&  \numprint{145}&\\ 
093 &\numprint{200}&\numprint{1207}&\numprint{200}&\numprint{1162}&X&X&X&X&X&  \numprint{143}&\\ 
095 &\numprint{15783}&\numprint{24663}&\numprint{510}&\numprint{1746}&-&X&-&-&X&  \numprint{9755}&\\ 
097 &\numprint{18096}&\numprint{28281}&\numprint{579}&\numprint{1995}&-&X&-&-&X&  \numprint{11185}&\\ 
099 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}
\centering

\caption{Detailed per instance results. The columns $n$,$m$ refer to the number of nodes and edges of the input graph, $n'$,$m'$ refer to the number of nodes and edges of the kernel graph after reductions have been applied exhaustively, and $|VC|$ refers to the size of the optimal vertex cover of the input graph. With X we denote if a solver has been solving an instance, and with - we denote if this has not been the case.}
\label{tab:detailedresults2}
\begin{tabular}{l@{\hskip 25pt} rrrr|ccccc|rc}
\toprule
inst\# & $n$ &$m$& $n'$& $m'$ & \texttt{MoMC} & \texttt{RMoMC} & \texttt{LSBnR} & \texttt{BnR} & \texttt{FullA} & $|VC|$ \\
                \midrule

101 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
103 &\numprint{15783}&\numprint{24663}&\numprint{513}&\numprint{1752}&-&X&-&-&X&  \numprint{9755}&\\ 
105 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
107 &\numprint{13590}&\numprint{21240}&\numprint{435}&\numprint{1500}&-&X&-&-&X&  \numprint{8400}&\\ 
109 &\numprint{66992}&\numprint{90970}&\numprint{20336}&\numprint{66350}&-&-&-&-&-&  &\\ 
111 &\numprint{450}&\numprint{17831}&\numprint{450}&\numprint{17831}&X&X&-&-&X&  \numprint{420}&\\ 
113 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
115 &\numprint{18096}&\numprint{28281}&\numprint{573}&\numprint{1986}&-&X&-&-&X&  \numprint{11185}&\\ 
117 &\numprint{18096}&\numprint{28281}&\numprint{582}&\numprint{2007}&-&X&-&-&X&  \numprint{11185}&\\ 
119 &\numprint{18096}&\numprint{28281}&\numprint{588}&\numprint{2016}&-&X&-&-&X&  \numprint{11185}&\\ 
121 &\numprint{18096}&\numprint{28281}&\numprint{579}&\numprint{1998}&-&X&-&-&X&  \numprint{11185}&\\ 
123 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
125 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
127 &\numprint{18096}&\numprint{28281}&\numprint{582}&\numprint{2001}&-&X&-&-&X&  \numprint{11185}&\\ 
129 &\numprint{15783}&\numprint{24663}&\numprint{507}&\numprint{1752}&-&X&-&-&X&  \numprint{9755}&\\ 
131 &\numprint{2980}&\numprint{5360}&\numprint{2179}&\numprint{6951}&X&-&-&-&X&  \numprint{1920}&\\ 
133 &\numprint{15783}&\numprint{24663}&\numprint{507}&\numprint{1746}&-&X&-&-&X&  \numprint{9755}&\\ 
135 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
137 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
139 &\numprint{18096}&\numprint{28281}&\numprint{579}&\numprint{1995}&-&X&-&-&X&  \numprint{11185}&\\ 
141 &\numprint{18096}&\numprint{28281}&\numprint{576}&\numprint{1995}&-&X&-&-&X&  \numprint{11185}&\\ 
143 &\numprint{18096}&\numprint{28281}&\numprint{582}&\numprint{2001}&-&X&-&-&X&  \numprint{11185}&\\ 
145 &\numprint{18096}&\numprint{28281}&\numprint{576}&\numprint{1989}&-&X&-&-&X&  \numprint{11185}&\\ 
147 &\numprint{18096}&\numprint{28281}&\numprint{567}&\numprint{1974}&-&X&-&-&X&  \numprint{11185}&\\ 
149 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
151 &\numprint{15783}&\numprint{24663}&\numprint{501}&\numprint{1728}&-&X&-&-&X&  \numprint{9755}&\\ 
153 &\numprint{29076}&\numprint{45570}&\numprint{2124}&\numprint{16266}&-&-&-&-&-&  &\\ 
155 &\numprint{26300}&\numprint{41500}&\numprint{500}&\numprint{3000}&-&-&X&-&X&  \numprint{16300}&\\ 
157 &\numprint{2980}&\numprint{5360}&\numprint{2169}&\numprint{6898}&X&-&-&-&X&  \numprint{1920}&\\ 
159 &\numprint{18096}&\numprint{28281}&\numprint{582}&\numprint{2004}&-&X&-&-&X&  \numprint{11185}&\\ 
161 &\numprint{138141}&\numprint{227241}&\numprint{41926}&\numprint{202869}&-&-&-&-&-&  &\\ 
163 &\numprint{18096}&\numprint{28281}&\numprint{582}&\numprint{2004}&-&X&-&-&X&  \numprint{11185}&\\ 
165 &\numprint{18096}&\numprint{28281}&\numprint{576}&\numprint{1995}&-&X&-&-&X&  \numprint{11185}&\\ 
167 &\numprint{15783}&\numprint{24663}&\numprint{510}&\numprint{1746}&-&X&-&-&X&  \numprint{9755}&\\ 
169 &\numprint{4768}&\numprint{8576}&\numprint{3458}&\numprint{11014}&-&-&-&-&-&  &\\ 
171 &\numprint{18096}&\numprint{28281}&\numprint{576}&\numprint{1989}&-&X&-&-&X&  \numprint{11185}&\\ 
173 &\numprint{56860}&\numprint{77264}&\numprint{17090}&\numprint{55568}&-&-&-&-&-&  &\\ 
175 &\numprint{3523}&\numprint{6446}&\numprint{2723}&\numprint{8570}&-&-&-&-&-&  &\\ 
177 &\numprint{5066}&\numprint{9112}&\numprint{3704}&\numprint{11797}&-&-&-&-&-&  &\\ 
179 &\numprint{15783}&\numprint{24663}&\numprint{504}&\numprint{1740}&-&X&-&-&X&  \numprint{9755}&\\ 
181 &\numprint{18096}&\numprint{28281}&\numprint{573}&\numprint{1989}&-&X&X&-&X&  \numprint{11185}&\\ 
183 &\numprint{72420}&\numprint{118362}&\numprint{30340}&\numprint{133872}&-&-&-&-&-&  &\\ 
185 &\numprint{3523}&\numprint{6446}&\numprint{2723}&\numprint{8568}&-&-&-&-&-&  &\\ 
187 &\numprint{4227}&\numprint{7734}&\numprint{3264}&\numprint{10286}&-&-&-&-&-&  &\\ 
189 &\numprint{7400}&\numprint{13600}&\numprint{5802}&\numprint{18212}&-&-&-&-&-&  &\\ 
191 &\numprint{4579}&\numprint{8378}&\numprint{3539}&\numprint{11137}&-&-&-&-&-&  &\\ 
193 &\numprint{7030}&\numprint{12920}&\numprint{5510}&\numprint{17294}&-&-&-&-&-&  &\\ 
195 &\numprint{1150}&\numprint{81068}&\numprint{1150}&\numprint{81068}&-&-&-&-&-&  &\\ 
197 &\numprint{1534}&\numprint{127011}&\numprint{1534}&\numprint{127011}&-&-&-&-&-&  &\\ 
199 &\numprint{1534}&\numprint{126163}&\numprint{1534}&\numprint{126163}&-&-&-&-&-&  &\\ 

\bottomrule
\end{tabular}
\end{table*}
Tables~\ref{tab:detailedresults1} and \ref{tab:detailedresults2} give an overview over the instances that each of the solver solved, about the kernel sizes as well as the optimal vertex cover size, if our full algorithm could solve the instance.
Overall, \texttt{MoMC} can solve 30 out of the 100 instances. 
Using reductions first, enables \texttt{RMoMC} to solve 68 instances. However, there are also instances that \texttt{MoMC} could solve, but \texttt{RMoMC} could not solve. 
In these case, the number of nodes has been reduced, but the number of edges actually increased. This is due to the \emph{Alternative} reduction, which in some cases can create more edges than initially present. This is why our full algorithm also runs MoMC on the input graph (in order to be able to solve those instances as well).
\texttt{LSBnR} can solve 55 out of the 100 instances. Here, priming the branch-and-reduce algorithm with an initial solution computed by local search has an important impact. Running the branch-and-reduce algorithm on the kernel without using local search can only solve 42 instances. In particular, using local search to find an initial bound helps to solve large instances in which the initial kernelization step does not reduce the graph fully. Our full algorithm \texttt{FullA} can solve 82 out of the 100 instances, and in particular, as expected, dominates each of the other configurations. 

On the private instances, our full algorithm solved 87, the second place (\textsf{peaty}~\cite{james_trimble_2019_3082356}) solved 77, the third place (\textsf{bogdan}~\cite{zbogdan_2019_3228802}) solved 76 instances (of 100 instances). Our solver dominates the other solvers: with one exeception, our algorithm solves all instances that peaty and bogdan can solve combined. The \textsf{peaty} solver used reductions to compute a problem kernel of the input and then used an unpublished maximum weight clique solver on the complement of each of the connected components of the kernel to assemble a solution. The clique solver is similar to \cite{DBLP:conf/aaai/LiQ10}, but is more general. Here, also local search is used to obtain a good initial solution. On the other hand, \textsf{bogdan} implemented a small suite of simple reductions (including vertex folding. isolated vertex, degree-one removal) together with a recent maximum clique solver by Szab\'o and Zavalnij~\cite{szabo2018different}. 


\section{Conclusion}
We presented the winning solver of the PACE 2019 Implementation Challenge Vertex Cover Track. Our algorithm uses a portfolio of techniques, including an aggressive kernelization strategy with all known reduction rules, local search, branch-and-reduce, and a state-of-the-art branch-and-bound solver. Of particular interest is that several of our techniques were \emph{not} from the literature on the vertex over problem: they were originally published to solve the (complementary) maximum independent set and maximum clique problems. Lastly, we performed extensive experiments to show the impact of the different solver techniques on the number of instances solved during the challenge. In particular, the results emphasize that data reductions play an important tool to boost the performance of the clique solver, and local search is highly effective to boost the performance of a branch-and-reduce solver for the independent set problem.

%\vfill 
%\pagebreak
%\
%\vfill
%\pagebreak
\bibliographystyle{plainurl}
\bibliography{references}
\vfill
\end{document}
